### Skip-gram 模型笔记

---

#### 1. 模型概述

Skip-gram 是一种用于训练词向量的模型，能够将词语映射到一个高维向量空间中，使得在该空间中语义相似的词距离更近。Skip-gram 模型的核心思想是**利用中心词预测上下文词**。

模型的目标是学习词的分布式表示，使得相似的词在向量空间中更接近。

---

#### 2. 模型结构

Skip-gram 模型中，每个词都有两个向量表示：

- **输入向量** $ v_w $：用于表示词作为中心词时的向量表示，存储在输入向量矩阵 $ V $ 中。
- **输出向量** $ v_c $：用于表示词作为上下文词时的向量表示，存储在输出向量矩阵 $ U $ 中。

假设词汇表大小为 $ |V| $（词汇量），向量维度为 $ d $，则：

- 输入向量矩阵 $ V $ 的维度为 $ |V| \times d $。
- 输出向量矩阵 $ U $ 的维度为 $ |V| \times d $。

---

#### 3. 模型的输入和输出

- **输入**：中心词（目标词）。模型将一个中心词输入到网络中，用于预测它的上下文词。
- **输出**：上下文词（正样本）。模型的输出是中心词的实际上下文词，预测上下文词的条件概率。

---

#### 4. 损失函数

Skip-gram 模型的目标是最大化中心词与上下文词之间的条件概率。给定一个中心词 $ w $ 和一个上下文词 $ c $，可以通过 softmax 函数计算条件概率：

$$
P(c | w) = \frac{\exp(v_c^T v_w)}{\sum_{c' \in V} \exp(v_{c'}^T v_w)}
$$

模型的总损失函数为所有词对的负对数似然：

$$
J = -\sum_{t=1}^{T} \sum_{-k \le j \le k, j \neq 0} \log P(w_{t+j} | w_t)
$$

其中，$ k $ 是上下文窗口大小，$ T $ 是文本中的总词数。

---

#### 5. 训练技巧：负采样

Skip-gram 的 softmax 计算代价很高，因此通过**负采样**来简化训练过程。负采样的主要思想是使用少数负样本来近似原始的条件概率，使得计算效率更高。

1. **正样本**：模型的目标是最大化中心词与实际上下文词的相似度。
   
   - 正样本损失：希望 $ \sigma(v_c^T v_w) \approx 1 $
   
     $$
     L_{\text{positive}} = -\log \sigma(v_c^T v_w)
     $$

2. **负样本**：随机选择一些不相关词作为负样本，尽量降低它们与中心词的相似性。
   
   - 负样本损失：希望 $ \sigma(-v_{c'}^T v_w) \approx 1 $
   
     $$
     L_{\text{negative}} = -\sum_{i=1}^N \log \sigma(-v_{c'}^T v_w)
     $$

负采样的总损失为：

$$
L = L_{\text{positive}} + L_{\text{negative}}
$$

---

#### 6. 训练过程中的参数更新

在训练过程中，模型会对词向量矩阵 $ V $ 和 $ U $ 中的向量进行更新：

1. **中心词输入向量** $ v_w $ 的更新：
   
   $$
   v_w \leftarrow v_w - \alpha \frac{\partial L}{\partial v_w}
   $$

2. **正样本上下文词输出向量** $ v_c $ 的更新：

   $$
   v_c \leftarrow v_c - \alpha \frac{\partial L}{\partial v_c}
   $$

3. **负样本上下文词输出向量** $ v_{c'} $ 的更新：

   $$
   v_{c'} \leftarrow v_{c'} - \alpha \frac{\partial L}{\partial v_{c'}}
   $$

其中 $ \alpha $ 为学习率。

---

#### 7. 数据预处理流程

在实际训练中，需要对文档数据进行预处理，以生成 Skip-gram 所需的 (中心词, 上下文词) 对：

1. **文本清洗**：去除标点、统一大小写。
2. **分词**：将文本拆分为单词列表。
3. **构建词汇表**：生成每个词的索引映射。
4. **生成 (中心词, 上下文词) 对**：根据上下文窗口大小提取中心词和上下文词对。

---

#### 8. 总结

Skip-gram 模型通过利用中心词预测上下文词来训练词向量，经过大量语料数据训练后，可以生成具有语义意义的词向量，使得相似词的向量距离更近。